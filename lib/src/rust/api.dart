// This file is automatically generated, so please do not edit it.
// @generated by `flutter_rust_bridge`@ 2.11.1.

// ignore_for_file: invalid_use_of_internal_member, unused_import, unnecessary_import

import 'frb_generated.dart';
import 'package:flutter_rust_bridge/flutter_rust_bridge_for_generated.dart';

// These function are ignored because they are on traits that is not defined in current crate (put an empty `#[frb]` on it to unignore): `clone`, `fmt`

// Rust type: RustOpaqueNom<flutter_rust_bridge::for_generated::RustAutoOpaqueInner<MnnLlm>>
abstract class MnnLlm implements RustOpaqueInterface {
  /// Apply chat template to messages
  ///
  /// # Arguments
  /// * `messages_json` - JSON array of messages: `[{"role":"user","content":"..."}]`
  ///
  /// # Returns
  /// Formatted prompt string with chat template applied
  Future<String> applyChatTemplate({required String messagesJson});

  /// Generate response using chat messages format (non-streaming)
  ///
  /// # Arguments
  /// * `messages_json` - JSON object: `{"messages":[{"role":"user","content":"..."}]}`
  Future<String> chat({required String messagesJson});

  /// Create a new LLM instance from a config path
  ///
  /// # Arguments
  /// * `config_path` - Path to llm_config.json or a directory containing it
  static MnnLlm create({required String configPath}) =>
      RustLib.instance.api.crateApiMnnLlmCreate(configPath: configPath);

  /// Decode tokens to text
  Future<String> detokenize({required List<int> tokens});

  /// Explicitly release the model and free all resources
  ///
  /// Call this before creating a new model to ensure clean memory state.
  /// After calling dispose, this instance should not be used.
  @override
  Future<void> dispose();

  /// Get the model's current configuration as JSON
  String dumpConfig();

  /// Erase history tokens in a range
  Future<void> eraseHistory({required BigInt begin, required BigInt end});

  /// Generate a complete response (non-streaming)
  ///
  /// Use this when you need the full response at once.
  Future<String> generate({required String prompt});

  /// Generate a streaming response
  ///
  /// Streams text chunks as they're generated.
  Stream<String> generateStream({required String prompt});

  /// Get context information (performance metrics)
  Future<ContextInfo?> getContextInfo();

  /// Get generated string from context
  Future<String> getGeneratedString();

  /// Get current history length (number of tokens)
  Future<BigInt> getHistoryLength();

  /// Get history tokens
  Future<Int32List> getHistoryTokens();

  /// Get output tokens from last generation
  Future<Int32List> getOutputTokens();

  /// Check if generation has stopped
  Future<bool> isStopped();

  /// Load the model into memory
  ///
  /// Must be called before generating responses.
  Future<bool> load();

  /// Reset conversation history
  ///
  /// Call this to start a new conversation.
  Future<void> reset();

  /// Set configuration options (JSON format)
  ///
  /// Call before load() to set options like use_mmap, tmp_path, backend, etc.
  ///
  /// # Arguments
  /// * `config_json` - JSON string with configuration options
  Future<void> setConfig({required String configJson});

  /// Enable or disable thinking mode for reasoning models
  Future<void> setThinking({required bool enabled});

  /// Encode text to tokens
  Future<Int32List> tokenize({required String text});

  /// Tune the model for optimal performance
  ///
  /// Call this after loading for best inference speed.
  Future<void> tune();

  /// Vision: Generate complete response with image(s) (non-streaming)
  Future<String> visionGenerate({required String prompt, required String imagePaths, int? width, int? height});

  /// Vision: Generate streaming response with image(s)
  ///
  /// # Arguments
  /// * `prompt` - Text prompt
  /// * `image_paths` - Comma-separated image paths or URLs
  /// * `width` - Optional image width (0 for auto)
  /// * `height` - Optional image height (0 for auto)
  /// * `sink` - StreamSink to receive text chunks
  Stream<String> visionGenerateStream({required String prompt, required String imagePaths, int? width, int? height});
}

/// Context information from the LLM (performance metrics)
class ContextInfo {
  final int promptTokens;
  final int decodeTokens;
  final PlatformInt64 promptUs;
  final PlatformInt64 decodeUs;

  const ContextInfo({
    required this.promptTokens,
    required this.decodeTokens,
    required this.promptUs,
    required this.decodeUs,
  });

  @override
  int get hashCode => promptTokens.hashCode ^ decodeTokens.hashCode ^ promptUs.hashCode ^ decodeUs.hashCode;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is ContextInfo &&
          runtimeType == other.runtimeType &&
          promptTokens == other.promptTokens &&
          decodeTokens == other.decodeTokens &&
          promptUs == other.promptUs &&
          decodeUs == other.decodeUs;
}
