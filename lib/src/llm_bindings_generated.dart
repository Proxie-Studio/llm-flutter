// Auto-generated by ffigen. Do not edit manually.
// ignore_for_file: type=lint

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
import 'dart:ffi' as ffi;

/// Auto-generated FFI bindings for MNN-LLM
class MnnLlmBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
  _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  MnnLlmBindings(ffi.DynamicLibrary dynamicLibrary)
    : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  MnnLlmBindings.fromLookup(
    ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName) lookup,
  ) : _lookup = lookup;

  /// Create a new LLM instance from a config path
  /// @param config_path Path to llm_config.json
  /// @return Handle to LLM instance, or NULL on error
  LlmHandle llm_create_ffi(ffi.Pointer<ffi.Char> config_path) {
    return _llm_create_ffi(config_path);
  }

  late final _llm_create_ffiPtr =
      _lookup<ffi.NativeFunction<LlmHandle Function(ffi.Pointer<ffi.Char>)>>(
        'llm_create_ffi',
      );
  late final _llm_create_ffi = _llm_create_ffiPtr
      .asFunction<LlmHandle Function(ffi.Pointer<ffi.Char>)>();

  /// Destroy an LLM instance and free resources
  /// @param handle LLM handle to destroy
  void llm_destroy_ffi(LlmHandle handle) {
    return _llm_destroy_ffi(handle);
  }

  late final _llm_destroy_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(LlmHandle)>>(
        'llm_destroy_ffi',
      );
  late final _llm_destroy_ffi = _llm_destroy_ffiPtr
      .asFunction<void Function(LlmHandle)>();

  /// Load the model weights
  /// Must be called after llm_create_ffi and before generation
  /// @param handle LLM handle
  /// @return true on success, false on failure
  bool llm_load_ffi(LlmHandle handle) {
    return _llm_load_ffi(handle);
  }

  late final _llm_load_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(LlmHandle)>>('llm_load_ffi');
  late final _llm_load_ffi = _llm_load_ffiPtr
      .asFunction<bool Function(LlmHandle)>();

  /// Tune/optimize the model for the current device
  /// Should be called after llm_load_ffi and before generation
  /// @param handle LLM handle
  void llm_tune_ffi(LlmHandle handle) {
    return _llm_tune_ffi(handle);
  }

  late final _llm_tune_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(LlmHandle)>>('llm_tune_ffi');
  late final _llm_tune_ffi = _llm_tune_ffiPtr
      .asFunction<void Function(LlmHandle)>();

  /// Reset conversation context/history
  /// @param handle LLM handle
  void llm_reset_ffi(LlmHandle handle) {
    return _llm_reset_ffi(handle);
  }

  late final _llm_reset_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(LlmHandle)>>(
        'llm_reset_ffi',
      );
  late final _llm_reset_ffi = _llm_reset_ffiPtr
      .asFunction<void Function(LlmHandle)>();

  /// Set configuration options (JSON format)
  /// Call before llm_load_ffi to set options like use_mmap, tmp_path
  /// @param handle LLM handle
  /// @param config_json JSON string with configuration
  void llm_set_config_ffi(LlmHandle handle, ffi.Pointer<ffi.Char> config_json) {
    return _llm_set_config_ffi(handle, config_json);
  }

  late final _llm_set_config_ffiPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(LlmHandle, ffi.Pointer<ffi.Char>)>
      >('llm_set_config_ffi');
  late final _llm_set_config_ffi = _llm_set_config_ffiPtr
      .asFunction<void Function(LlmHandle, ffi.Pointer<ffi.Char>)>();

  /// Dump current config as JSON string
  /// @param handle LLM handle
  /// @return JSON config string (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_dump_config_ffi(LlmHandle handle) {
    return _llm_dump_config_ffi(handle);
  }

  late final _llm_dump_config_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(LlmHandle)>>(
        'llm_dump_config_ffi',
      );
  late final _llm_dump_config_ffi = _llm_dump_config_ffiPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(LlmHandle)>();

  /// Set thinking mode (for models that support chain-of-thought)
  /// @param handle LLM handle
  /// @param thinking Enable or disable thinking mode
  void llm_set_thinking_ffi(LlmHandle handle, bool thinking) {
    return _llm_set_thinking_ffi(handle, thinking);
  }

  late final _llm_set_thinking_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(LlmHandle, ffi.Bool)>>(
        'llm_set_thinking_ffi',
      );
  late final _llm_set_thinking_ffi = _llm_set_thinking_ffiPtr
      .asFunction<void Function(LlmHandle, bool)>();

  /// Generate a response (non-streaming)
  /// @param handle LLM handle
  /// @param prompt User prompt string
  /// @return Generated response (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_generate_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> prompt,
  ) {
    return _llm_generate_ffi(handle, prompt);
  }

  late final _llm_generate_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
        >
      >('llm_generate_ffi');
  late final _llm_generate_ffi = _llm_generate_ffiPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
      >();

  /// Generate a response with streaming callback
  /// @param handle LLM handle
  /// @param prompt User prompt string
  /// @param callback Function called for each token
  /// @param user_data User data passed to callback
  /// @return true on success, false on failure
  bool llm_generate_stream_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> prompt,
    LlmStreamCallback callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _llm_generate_stream_ffi(handle, prompt, callback, user_data);
  }

  late final _llm_generate_stream_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Bool Function(
            LlmHandle,
            ffi.Pointer<ffi.Char>,
            LlmStreamCallback,
            ffi.Pointer<ffi.Void>,
          )
        >
      >('llm_generate_stream_ffi');
  late final _llm_generate_stream_ffi = _llm_generate_stream_ffiPtr
      .asFunction<
        bool Function(
          LlmHandle,
          ffi.Pointer<ffi.Char>,
          LlmStreamCallback,
          ffi.Pointer<ffi.Void>,
        )
      >();

  /// Check if generation has stopped
  /// @param handle LLM handle
  /// @return true if stopped
  bool llm_stopped_ffi(LlmHandle handle) {
    return _llm_stopped_ffi(handle);
  }

  late final _llm_stopped_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(LlmHandle)>>(
        'llm_stopped_ffi',
      );
  late final _llm_stopped_ffi = _llm_stopped_ffiPtr
      .asFunction<bool Function(LlmHandle)>();

  /// Get generated string from the context
  /// @param handle LLM handle
  /// @return Generated string (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_get_generated_string_ffi(LlmHandle handle) {
    return _llm_get_generated_string_ffi(handle);
  }

  late final _llm_get_generated_string_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(LlmHandle)>>(
        'llm_get_generated_string_ffi',
      );
  late final _llm_get_generated_string_ffi = _llm_get_generated_string_ffiPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(LlmHandle)>();

  /// Apply chat template to a user message
  /// @param handle LLM handle
  /// @param user_content User message content
  /// @return Formatted prompt (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_apply_chat_template_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> user_content,
  ) {
    return _llm_apply_chat_template_ffi(handle, user_content);
  }

  late final _llm_apply_chat_template_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
        >
      >('llm_apply_chat_template_ffi');
  late final _llm_apply_chat_template_ffi = _llm_apply_chat_template_ffiPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
      >();

  /// Apply chat template to a JSON array of messages
  /// Expected format: [{"role": "user", "content": "..."}, ...]
  /// @param handle LLM handle
  /// @param messages_json JSON array of messages
  /// @return Formatted prompt (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_apply_chat_template_json_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> messages_json,
  ) {
    return _llm_apply_chat_template_json_ffi(handle, messages_json);
  }

  late final _llm_apply_chat_template_json_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
        >
      >('llm_apply_chat_template_json_ffi');
  late final _llm_apply_chat_template_json_ffi =
      _llm_apply_chat_template_json_ffiPtr
          .asFunction<
            ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Pointer<ffi.Char>)
          >();

  /// Encode text to token IDs
  /// @param handle LLM handle
  /// @param text Text to encode
  /// @param output Output buffer for token IDs (can be NULL to get required size)
  /// @param max_tokens Maximum tokens to write
  /// @return Number of tokens written, or required size if output is NULL
  int llm_tokenizer_encode_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> text,
    ffi.Pointer<ffi.Int32> output,
    int max_tokens,
  ) {
    return _llm_tokenizer_encode_ffi(handle, text, output, max_tokens);
  }

  late final _llm_tokenizer_encode_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Size Function(
            LlmHandle,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Int32>,
            ffi.Size,
          )
        >
      >('llm_tokenizer_encode_ffi');
  late final _llm_tokenizer_encode_ffi = _llm_tokenizer_encode_ffiPtr
      .asFunction<
        int Function(
          LlmHandle,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Int32>,
          int,
        )
      >();

  /// Decode a single token ID to string
  /// @param handle LLM handle
  /// @param token Token ID to decode
  /// @return Decoded string (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_tokenizer_decode_ffi(LlmHandle handle, int token) {
    return _llm_tokenizer_decode_ffi(handle, token);
  }

  late final _llm_tokenizer_decode_ffiPtr =
      _lookup<
        ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(LlmHandle, ffi.Int32)>
      >('llm_tokenizer_decode_ffi');
  late final _llm_tokenizer_decode_ffi = _llm_tokenizer_decode_ffiPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(LlmHandle, int)>();

  /// Check if a token is a stop token
  /// @param handle LLM handle
  /// @param token Token ID to check
  /// @return true if token is a stop token
  bool llm_is_stop_ffi(LlmHandle handle, int token) {
    return _llm_is_stop_ffi(handle, token);
  }

  late final _llm_is_stop_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(LlmHandle, ffi.Int32)>>(
        'llm_is_stop_ffi',
      );
  late final _llm_is_stop_ffi = _llm_is_stop_ffiPtr
      .asFunction<bool Function(LlmHandle, int)>();

  /// Get current history length
  /// @param handle LLM handle
  /// @return Number of history entries
  int llm_get_current_history_ffi(LlmHandle handle) {
    return _llm_get_current_history_ffi(handle);
  }

  late final _llm_get_current_history_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(LlmHandle)>>(
        'llm_get_current_history_ffi',
      );
  late final _llm_get_current_history_ffi = _llm_get_current_history_ffiPtr
      .asFunction<int Function(LlmHandle)>();

  /// Erase history in range [begin, end)
  /// @param handle LLM handle
  /// @param begin Start index (inclusive)
  /// @param end End index (exclusive)
  void llm_erase_history_ffi(LlmHandle handle, int begin, int end) {
    return _llm_erase_history_ffi(handle, begin, end);
  }

  late final _llm_erase_history_ffiPtr =
      _lookup<
        ffi.NativeFunction<ffi.Void Function(LlmHandle, ffi.Size, ffi.Size)>
      >('llm_erase_history_ffi');
  late final _llm_erase_history_ffi = _llm_erase_history_ffiPtr
      .asFunction<void Function(LlmHandle, int, int)>();

  /// Check if KV cache reuse is enabled
  /// @param handle LLM handle
  /// @return true if KV cache reuse is enabled
  bool llm_reuse_kv_ffi(LlmHandle handle) {
    return _llm_reuse_kv_ffi(handle);
  }

  late final _llm_reuse_kv_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(LlmHandle)>>(
        'llm_reuse_kv_ffi',
      );
  late final _llm_reuse_kv_ffi = _llm_reuse_kv_ffiPtr
      .asFunction<bool Function(LlmHandle)>();

  /// Get context info (token counts and timing metrics)
  /// @param handle LLM handle
  /// @return Context info structure
  LlmContextInfo llm_get_context_info_ffi(LlmHandle handle) {
    return _llm_get_context_info_ffi(handle);
  }

  late final _llm_get_context_info_ffiPtr =
      _lookup<ffi.NativeFunction<LlmContextInfo Function(LlmHandle)>>(
        'llm_get_context_info_ffi',
      );
  late final _llm_get_context_info_ffi = _llm_get_context_info_ffiPtr
      .asFunction<LlmContextInfo Function(LlmHandle)>();

  /// Format a vision prompt with image(s)
  /// @param handle LLM handle
  /// @param prompt Text prompt
  /// @param image_paths Image path(s), comma-separated for multiple
  /// @param width Optional image width (0 for auto)
  /// @param height Optional image height (0 for auto)
  /// @return Formatted vision prompt (must be freed with llm_free_string_ffi)
  ffi.Pointer<ffi.Char> llm_format_vision_prompt_ffi(
    LlmHandle handle,
    ffi.Pointer<ffi.Char> prompt,
    ffi.Pointer<ffi.Char> image_paths,
    int width,
    int height,
  ) {
    return _llm_format_vision_prompt_ffi(
      handle,
      prompt,
      image_paths,
      width,
      height,
    );
  }

  late final _llm_format_vision_prompt_ffiPtr =
      _lookup<
        ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
            LlmHandle,
            ffi.Pointer<ffi.Char>,
            ffi.Pointer<ffi.Char>,
            ffi.Int32,
            ffi.Int32,
          )
        >
      >('llm_format_vision_prompt_ffi');
  late final _llm_format_vision_prompt_ffi = _llm_format_vision_prompt_ffiPtr
      .asFunction<
        ffi.Pointer<ffi.Char> Function(
          LlmHandle,
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>,
          int,
          int,
        )
      >();

  /// Free a string returned by llm functions
  /// @param s String to free
  void llm_free_string_ffi(ffi.Pointer<ffi.Char> s) {
    return _llm_free_string_ffi(s);
  }

  late final _llm_free_string_ffiPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ffi.Char>)>>(
        'llm_free_string_ffi',
      );
  late final _llm_free_string_ffi = _llm_free_string_ffiPtr
      .asFunction<void Function(ffi.Pointer<ffi.Char>)>();
}

/// Context info structure for performance metrics
final class LlmContextInfo extends ffi.Struct {
  @ffi.Int32()
  external int prompt_tokens;

  @ffi.Int32()
  external int decode_tokens;

  @ffi.Int64()
  external int prompt_us;

  @ffi.Int64()
  external int decode_us;
}

/// Opaque handle type for LLM instance
typedef LlmHandle = ffi.Pointer<ffi.Void>;

/// Callback type for streaming responses
/// @param token The token string pointer
/// @param len Length of the token string
/// @param user_data User provided data pointer
/// @return true to continue generation, false to stop
typedef LlmStreamCallback =
    ffi.Pointer<ffi.NativeFunction<LlmStreamCallbackFunction>>;
typedef LlmStreamCallbackFunction =
    ffi.Bool Function(
      ffi.Pointer<ffi.Char> token,
      ffi.Size len,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef DartLlmStreamCallbackFunction =
    bool Function(
      ffi.Pointer<ffi.Char> token,
      int len,
      ffi.Pointer<ffi.Void> user_data,
    );
