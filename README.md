# MNN-LLM Flutter

On-device Large Language Model inference for Flutter using MNN backend.

## Features

- ðŸš€ On-device inference (no cloud required)
- ðŸ“± Android arm64 support
- ðŸ”„ Streaming text generation
- ðŸ’¬ Chat interface with Qwen3-4B model
- ðŸ§  Memory-mapped file loading for reduced RAM usage

## Prerequisites

- Flutter SDK 3.10+
- Rust toolchain with Android targets
- Android NDK r26+
- ~4GB storage for model weights

## Quick Start

### 1. Clone and Setup

```bash
git clone https://github.com/Proxie-Studio/llm.git
cd llm
git checkout feat/flutter
```

### 2. Build Native Libraries

```bash
# Install Rust Android targets
rustup target add aarch64-linux-android

# Build for Android arm64
cd rust/mnn_llm
./scripts/build_flutter.sh arm64-v8a
```

This builds the Rust FFI wrapper and copies libraries to `android/app/src/main/jniLibs/arm64-v8a/`.

### 3. Download Model

Download the Qwen3-4B-Instruct MNN model (~2.7GB):

```bash
# On your Android device or emulator
adb shell mkdir -p /data/local/tmp/Qwen3-4B-Instruct-2507-MNN
adb push /path/to/model/* /data/local/tmp/Qwen3-4B-Instruct-2507-MNN/
```

The model directory should contain:
- `llm_config.json` - Model configuration
- `llm.mnn` - Model weights
- `tokenizer.txt` - Tokenizer vocabulary

### 4. Run the App

```bash
# Get Flutter dependencies
flutter pub get

# Run on Android device/emulator
flutter run
```

## Project Structure

```
lib/
â”œâ”€â”€ main.dart                    # Flutter chat UI
â”œâ”€â”€ llm_flutter.dart            # Library exports
â””â”€â”€ src/
    â”œâ”€â”€ llm.dart                # High-level Dart API
    â””â”€â”€ llm_bindings_generated.dart  # Auto-generated FFI bindings

rust/mnn_llm/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs                  # Rust library
â”‚   â”œâ”€â”€ llm.rs                  # LLM wrapper
â”‚   â””â”€â”€ ffi.rs                  # C FFI exports
â”œâ”€â”€ include/
â”‚   â””â”€â”€ mnn_llm.h              # C header for ffigen
â””â”€â”€ scripts/
    â””â”€â”€ build_flutter.sh       # Build script

android/app/src/main/jniLibs/arm64-v8a/
â”œâ”€â”€ libc++_shared.so           # C++ runtime
â”œâ”€â”€ libMNN.so                  # MNN core
â”œâ”€â”€ libMNN_Express.so          # MNN Express API
â”œâ”€â”€ libllm.so                  # MNN LLM engine
â”œâ”€â”€ libMNNOpenCV.so            # OpenCV support
â””â”€â”€ libmnn_llm_rust.so         # Rust FFI wrapper
```

## API Usage

### Basic Generation

```dart
import 'package:llm_flutter/llm_flutter.dart';

// Create LLM instance
final llm = Llm('/path/to/llm_config.json');

// Load model
llm.load();
llm.tune();

// Generate response
final response = llm.generate('Hello, how are you?');
print(response);

// Cleanup
llm.dispose();
```

### Streaming Generation

```dart
// Stream tokens as they're generated
await for (final token in llm.generateStream('Tell me a story')) {
  stdout.write(token);
}
```

### Configuration

```dart
// Enable memory-mapped loading (reduces RAM usage)
final llm = Llm(
  '/path/to/llm_config.json',
  useMmap: true,
  tmpPath: '/path/to/cache/dir',
);

// Enable thinking mode (chain-of-thought)
llm.setThinking(true);
```

## Regenerating FFI Bindings

If you modify the Rust FFI interface:

```bash
# Regenerate Dart bindings from C header
dart run ffigen --config ffigen.yaml
```

## Troubleshooting

### App crashes on startup
- Ensure all native libraries are in `jniLibs/arm64-v8a/`
- Check library loading order (libc++_shared.so must load first)

### Out of memory errors
- Enable mmap: `Llm(path, useMmap: true)`
- Ensure device has 4GB+ RAM
- Use a smaller model variant

### Model not found
- Verify model path: `/data/local/tmp/Qwen3-4B-Instruct-2507-MNN/llm_config.json`
- Check file permissions: `adb shell chmod -R 755 /data/local/tmp/Qwen3-4B-Instruct-2507-MNN`

### Emulator issues
- Emulators have limited storage; use a real device for best results
- Allocate at least 6GB storage to the emulator AVD

## License

MIT
